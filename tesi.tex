\documentclass[a4paper, 12pt]{article}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} %per aggiungere le immagini
\usepackage{listings} %per aggiungere codice sorgente
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{float}
\usepackage{frontespizio}
\lstset{basicstyle=\ttfamily}
% interruzione di pagina tra le section
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

\newcommand{\citazioni}{cit}
\newcommand{\coautori}{coa}
\newcommand{\altroindice}{t}
\newcommand{\pagerankModificato}{Pmod}
\newcommand{\pesiPagerankModificato}{pmod}
\newcommand{\pagerank}{P}
\newcommand{\pesiPagerank}{p}
\newcommand{\citerank}{CiteRank}


\title{Analisi di reti di citazioni tramite PageRank e relazioni di collaborazione}
\author{Lorenzo Gabriele}

\pagenumbering{arabic}
\linespread{2.0}

\begin{document}

\tableofcontents
\clearpage

\section{Introduzione}
Insieme con l'enorme quantità di dati riguardanti le pubblicazioni scientifiche disponibile nei database online, come Dblp nell'ambito dell'informatica e Pubmed in quello delle scienze biomediche, è nata l'idea di utilizzare algoritmi matematici e statistici per ricavare dati utili sulle pubblicazioni e sui loro autori. \\
Tra i metodi di analisi oggi utilizzati vi è l'algoritmo PageRank. Nato inizialmente per valutare la popolarità delle pagine web, viene utilizzato per stimare la probabilità di un autore di essere citato da altri autori al fine di valutarne la visibilità.
\par Si propone un algortimo per stimare la visibilità degli autori basato su una modifica dell'algoritmo PageRank, affinché essa sia valutata al di fuori dalla loro rete di collaborazione.

\section{Concetti introduttivi}
\subsection{Il Mining Bibliografico}
Il lavoro proposto si inserisce all'interno dell'ambito della branca del data mining che va sotto il nome di mining bibliografico. Tale branca nasce con l'obiettivo di ottenere informazioni statisticamente utili sugli articoli scientifici, sui loro autori, sulle loro collaborazioni a partire dal grande quantitativo di dati presente online. \\
I database bibliografici online, come DBLP in informatica e PubMed in scienze mediche, contengono abbondanti informazioni sulle pubblicazioni di ricerca in diversi campi. Ciascuno di questi database costituisce una gigantesca rete di informazioni, che collega in modo complesso documenti di ricerca, autori, conferenze, riviste e possibilmente anche informazioni sulle citazioni e fornisce un terreno fertile per l'analisi delle reti di informazioni.
\subsubsection{Bibliometria e bibliomining}
Nata ben prima del data mining è la bibliometria. La bibliometria, in senso lato, è un'analisi statistica di pubblicazioni scritte, come libri o articoli. I metodi bibliometrici sono frequentemente usati nel campo della biblioteconomia e della scienza dell'informazione. La bibliometria viene utilizzata per fornire un'analisi quantitativa della letteratura scientifica o per valutare la spesa di bilancio. L' analisi delle citazioni è un metodo bibliometrico comunemente usato che si basa sulla costruzione del grafico delle citazioni, una rappresentazione di rete o grafico delle citazioni tra documenti. Molti campi di ricerca utilizzano metodi bibliometrici per esplorare l'impatto del loro campo, l'impatto di una serie di ricercatori, l'impatto di un particolare articolo o per identificare documenti particolarmente di impatto all'interno di uno specifico campo di ricerca.
\par
Il bibliomining \cite{NICHOLSON2006785} è l'uso di una combinazione di data mining e bibliometria allo scopo di analizzare i servizi di libreria. Il termine è stato creato nel 2003 da Scott Nicholson, Assistant Professor della Syracuse University School of Information Studies, al fine di distinguere il data mining bibliografico da altri tipi di data mining.
\subsubsection{Reti di citazioni}
Una delle pratiche di mining bibliografico più diffuse è quella di analisi delle reti di citazioni. \\
Una rete di citazione è un grafo orientato in cui i nodi sono rappresentati da articoli (o autori di articoli) e gli archi sono costituiti dalle citazioni presenti tra gli articoli. Nel caso del grafo degli articoli la relazione di citazione rappresenta un arco non pesato in cui sono possibili due soli stati per l'arco: citazione presente e citazione non presente.
Nel caso, invece, del grafo degli autori, gli archi tra gli autori sono di tipo pesato. \\
Il peso degli archi è definito come di seguito: per due generici nodi $A$ e $B$, l'arco orientato da $A$ a $B$ con peso $n$ rappresenta un numero totale di citazioni di articoli scritti dall'autore $A$ ad articoli scritti dall'autore $B$ pari ad $n$.
\subsubsection{Reti di collaborazione}
Semanticamente vicine alle reti di citazioni, le reti di collaborazione sono un concetto atto a quantificare la collaborazione tra autori.
Una rete di collaborazione è un grafo orientato in cui i nodi sono rappresentati da autori e gli archi sono costituiti dalle collaborazioni tra gli autori. Il concetto di collaborazione si può quantificare, con buona approssimazione, utilizzando le coauthorships, ovvero contando il numero di articoli. Nel caso del grafo degli articoli la relazione di citazione rappresenta un arco non pesato in cui sono possibili due soli stati per l'arco: citazione presente e citazione non presente.
Nel caso, invece, del grafo degli autori, gli archi tra gli autori sono di tipo pesato. \\
Il peso degli archi è definito come di seguito: per due generici nodi $A$ e $B$, l'arco orientato da $A$ a $B$ con peso $n$ rappresenta un numero totale di citazioni di articoli scritti dall'autore $A$ ad articoli scritti dall'autore $B$ pari ad $n$.

\subsection{Basi di dati di grafi}
Per l'elaborazione dei dati relativi alle varie relazioni tra autori e articoli, si è visto naturale l'utilizzo delle basi di dati di grafi. \\
Questo particolare tipo di base di dati offre le ottimizzazioni necessarie a rappresentare un grande numero di relazioni tra entità, in modo dinamico, senza dover racchiudere lo schema in convenzionali tabelle. Offre, altresì, convenienti operazioni e algoritmi pre-implementati per l'analisi e l'elaborazione dei grafi, tra i quali risulta presente anche l'algoritmo PageRank.
\par
La principale caratteristica che differenzia le basi di dati di grafi rispetto alle classiche basi di dati relazionali è che le operazioni di join, mentre nel modello relazionale sono svolte a tempo di interrogazione, facendo il matching delle chiavi primarie e delle chiavi esterne di tutte le righe della nelle tabelle interrogate, operazione tra l'altro pesante sia dal punto di vista computazionale che dell'utilizzo di memoria, sono ottimizzate conservando le relazioni all'interno dei nodi stessi. Viene incrementata, così, la dimensione su disco del database, a vantaggio di una ridotta complessità delle operazioni di join.
Le basi di dati di grafi ricadono nella categoria delle basi di dati NoSQL, ovvero nella categoria delle basi di dati create con l'intento di superare i limiti delle tradizionali basi di dati relazionali. Mentre condividono con gli altri database NoSQL l'idea che non debba essere necessario definire staticamente il modello della base di dati in modo preventivo al suo utilizzo, differiscono per quanto riguarda la strategia di memorizzazione dei dati.
\par
Il classico linguaggio SQL, sebbene abbia potenza sufficiente a rappresentare la maggior parte delle interrogazioni sui grafi, non essendo stato pensato per tale utilizzo, risulta in interrogazioni eccessivamente complicate e prone ad errori. Per ovviare alle difficoltà di utilizzo di SQL nel dominio dei grafi i graph database sono dotate di linguaggi di interrogazione specializzati per rappresentare i nodi e gli archi dei grafi.

\subsubsection{Il Graph DBMS Neo4J}
\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/neo4j.png}
  \caption{Neo4J Developement Environment}
\end{figure}
Tra le opzioni presenti, la scelta è ricaduta su Neo4J \cite{neo4j}, il dbms di grafi più diffuso sulla piattaforma Java. Esso possiede due diverse distribuzioni: neo4j-community e neo4j-enterprise. La distribuzione enterprise è pensata per l'utilizzo in cloud, offrendo soluzioni con un maggiore parallelismo per l'esecuzione degli algoritmi, la possibilità di eseguire algoritmi su cluster, e, a dire degli autori, un runtime per l'esecuzione delle query in linguaggio Cypher del 70\% più veloce. \\
Non essendoci la necessità di esecuzione in cloud si è scelto di utilizzare la distribuzione community. \\
Neo4J introduce un suo linguaggio di querying per l'interazione dell'utente con il database. Tale linguaggio nasce con l'obiettivo di essere facilmente leggibile e comprensibile dagli utenti, fornendo, allo stesso tempo, le features necessarie a gestire convenientemente i grafi, a differenza del classico linguaggio SQL.
\par
Di seguito si fornisce una breve panoramica delle possibilità del linguaggio Cypher.
\subsubsection{Il linguaggio Cypher}
I nodi del grafo sono rappresentati, nelle query Cypher, da un nome racchiuso tra due parentesi tonde (che rappresentano la circonferenza con cui si suole rappresentare i nodi nelle rappresentazioni grafiche dei grafi). La ricerca nel database si effettua attraverso l'istruzione MATCH seguita dall'oggetto da cercare.
MATCH (a) seleziona tutti i nodi del grafo, che vengono rappresentati nella query dal nome "a". Sono selezionati tutti i nodi del grafo perché non sono presenti vincoli.
MATCH (a: Author) seleziona, invece, i nodi che hanno come tipo Author. I tipi non vanno dichiarati precedentemente all'uso, ma lo schema è generato dinamicamente. Il tipo Author farà parte del database quando sarà creato il primo nodo di tale tipo.
\`E possibile filtrare ulteriormente utilizzando vincoli sui campi che possiede un dato nodo:
MATCH (a: Author {name: "Giuseppe Rossi"}) seleziona tutti i nodi di tipo Author per cui esista un campo denominato ``name'' il cui valore sia la stringa "Giuseppe Rossi". Data l'assenza di una fase separata per la definizione dello schema della base di dati, nulla vieta di avere nodi di tipi diversi che abbiano campi con nomi e tipi diversi. Tuttavia, questa diversificazione può essere deleteria ad una buona gestione di una base di dati. A tal proposito Neo4j permette la definizione di vincoli che permettano, oltre che a mantenere dati i coerenti, a rendere efficienti le interrogazioni attraverso l'utilizzo di indici.
Un vincolo utile per l'ottimizzazione delle query è quello di unicità. \`E possibile aggiungere il vincolo di unicità attraverso una query Cypher:
\begin{lstlisting}[keepspaces=true]
  CREATE CONSTRAINT ON (book:Book)
    ASSERT book.isbn IS UNIQUE
\end{lstlisting}
Attraverso questo comando si può istruire il database che la il campo denominato isbn dei nodi di tipo Book è unico. Per unico si intende che in tutta la base di dati non esistono due nodi distinti di tipo Book con lo stesso isbn. Essendo presente questo vincolo, la base di dati procede all'indicizzazione del campo isbn. In tal modo le query successive di MATCH sul campo isbn avranno la stessa velocità di quelle effettuate sull'ID del nodo.
\par
La creazione dei nodi avviene attraverso la parola chiave CREATE. Attraverso la parola chiave CREATE è possibile aggiungere sia nodi che relazioni alla base di dati. \\
Per creare un nodo si usa una sintassi simile a quella usata per MATCH:
\begin{lstlisting}[keepspaces=true]
  CREATE (a: Author {name: "Giuseppe Rossi"})
\end{lstlisting}
Per creare, invece, una relazione tra due nodi esistenti, si utilizza la seguente sintassi:
\begin{lstlisting}[keepspaces=true]
  MATCH (a: Author {name: "Giuseppe Rossi"}),
    (b: Author {name: "Francesco Bianchi"})
  CREATE (a)-[:COAUTHOR]->(b)
\end{lstlisting}
In tal modo si definisce un arco orientato dal nodo a al nodo b. \\
Anche le relazioni possono avere, come i nodi, dei campi. Questi si definiscono utilizzando una sintassi simile a quella utilizzata per definire campi nei nodi:
\begin{lstlisting}[keepspaces=true]
  MATCH (a: Author {name: "Giuseppe Rossi"}),
        (b: Author {name: "Francesco Bianchi"})
  CREATE (a)-[:COAUTHOR {times: 1}]->(b)
\end{lstlisting}
Per poter inserire qualcosa che si avvicini alla nozione di pesi di un arco, è utile poter inserire un contatore numerico, aggiornandolo ad ogni nuovo matching.
Per questo scopo esiste una seconda parola chiave del linguaggio Cypher, la parola chiave ``MERGE''. Essa equivale a fare match su un nodo o su una relazione in fase di creazione; se il matching esiste allora si esegue una sotto-query relativa al matching, se non esiste, invece, si esegue una sotto-query relativa alla creazione.
\begin{lstlisting}[keepspaces=true]
  MATCH (a: Author {name: "Giuseppe Rossi"}),
        (b: Author {name: "Francesco Bianchi"})
  MERGE (a)-[c:COAUTHOR]->(b)
  ON CREATE SET c.times = 1
  ON MATCH  SET c.times = c.times + 1
\end{lstlisting}
Eseguendo questa query ripetutamente è possibile incrementare il contatore "times" associato all'arco. \`E questo il caso, ad esempio dell'importazione batch di nodi e archi del grafo.
Neo4J permette l'importazione diretta attraverso la parola chiave Cypher LOAD CSV:
\begin{lstlisting}
LOAD CSV FROM 'file://artists.csv' AS line
CREATE (:Artist { name: line[1], year: toInteger(line[2])})
\end{lstlisting}
Utilizzando la parola chiave LOAD CSV è possibile fare il parsing di un file di testo comma separated values e creare nodi durante la scansione. \\
Il problema di tale pratica è che l'intera operazione viene fatta all'interno della stessa transazione. Nel caso di file csv di grandissime dimensioni questo può riempire la memoria. \`E necessario uno strumento per dividere l'importazione in tante transazioni separate. \\
Il linguaggio Cypher mette a disposizione la funzione ``USING PERIODIC COMMIT'' che, ricevendo un numero intero rappresentante la cardinalità di righe da computare prima di eseguire il commit, divide l'operazioni in tante, più piccole, e dunque gestibili, transazioni.

\subsubsection{Le estensioni di Neo4J}
Il DBMS Neo4J permette la creazione di estensioni per aggiungere funzioni utilizzabili attraverso le query Cypher. \\
Tra le estensioni più utilizzate su piattaforma Neo4J vi sono quelle del pacchetto Apoc. \\
Queste forniscono implementazioni diverse dell'algoritmo PageRank, e funzioni per lanciare query batch su grandi quantità di dati.
Per il calcolo del PageRank, le librerie Apoc offrono diverse versioni:
\begin{enumerate}
  \item pageRank \\
  Questa è la versione classica dell'algoritmo che calcola il pageRank sui nodi selezionati passati come parametro.
  \item pageRankWithConfig \\
  Questa versione dell'algoritmo riceve, inoltre, una configurazione sul numero di iterazioni e sulle label delle relazioni da considerare nel calcolo.
  \item pageRankWithCypher \\
  Questa versione, la più generale tra quelle disponibili nella libreria, riceve una query Cypher che deve ritornare, un insieme di nodi ed un insieme di archi con associati dei pesi. Tali informazioni saranno usate nel calcolo del PageRank.
  \begin{lstlisting}
CALL apoc.algo.pageRankWithCypher({
  iterations:5, write:true, 
  node_cypher:
  'MATCH (p:Person) return id(p) as id',
  rel_cypher: 
  'MATCH (p:Person)-->()<--(o:Person)
  return id(p) as source,
  id(o) as target, 1 as weight'
});
\end{lstlisting} 

\par
Sebbene le basi di dati di grafi, come Neo4J, offrano molte utilità per l'elaborazione dei grafi, tra cui linguaggi molto potenti per l'interrogazione, non hanno le performance adeguate a gestire grafi di milioni di nodi e centinaia di milioni di archi. \\
\`E stata, dunque, implementata una soluzione custom per importare i dati e per eseguire il PageRank sopra tali dati.
\end{enumerate}

\section{La tecnica proposta}
La tecnica proposta utilizza l'algoritmo PageRank per fornire una stima della variazione della visibilità degli autori considerando la variazione di impatto della rete di collaborazione. 

\subsection{Nozioni preliminari}
Si danno di seguito le nozioni preliminari e le definizioni utilizzate nei paragrafi successivi.
\subsubsection{Grafo di citazioni}
Dato un insieme di Nodi $V$ e un insieme di archi orientati pesati $E$ si definisce $G = (V,E)$ il grafo delle citazioni. Tale grafo associa agli archi un peso $p_e\in\mathbb{N}$ tale che per ogni coppia di nodi $v_i, v_j \in V$, $p_{ij}$ corrisponde al numero di citazioni totali del nodo $v_i$ incidenti sul nodo $v_j$.
\subsubsection{Grafo multipeso}
Dato un insieme di Nodi $V$ e un insieme di archi orientati pesati $E$ si definisce $G^m = (V,E^m)$ il grafo delle citazioni arricchito. Tale grafo associa agli archi una famiglia di pesi $p^m_e\in\mathbb{N}$ tale che per ogni coppia di nodi $v_i, v_j \in V$ e per ogni statistica $s_k$ con $k \in \{0,1..w \}$, $p^m_{ijk}$ corrisponde alla statistica $k$-esima del nodo $v_i$ incidente sul nodo $v_j$.

\subsubsection{L'algoritmo PageRank}
PageRank \cite{Page98thepagerank} è un algoritmo introdotto da Google per classificare le pagine web. 
Nasce dalla necessità del motore di ricerca di avere un ordinamento delle pagine web rispetto alla loro importanza. Esso funziona contando il numero di link delle pagine web per avere una stima approssimativa della qualità e dell'importanza di un sito web. L'assunzione su cui questo si basa è quella che, tanto più una pagina è importante, più tende a ricevere collegamenti dalle altre pagine.
\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/pagerank.jpg}
  \caption{Rappresentazione grafica del ranking con PageRank}
\end{figure}
Dal punto di vista matematico il PageRank di una pagina è definito in modo ricorsivo e dipende dal numero e dalla metrica PageRank di tutte le pagine che la collegano (archi entranti). Una pagina a cui sono collegate molte pagine con un PageRank elevato riceve un livello elevato.
\subsubsection{Storia}
Larry Page e Sergey Brin, i due co-fondatori di Google, svilupparono l'algoritmo PageRank nel 1996 come parte di un progetto di ricerca dell'università di Standford riguardo un nuovo tipo di motore di ricerca. Fu proprio di Sergey Brin l'idea che il World Wide Web dovesse essere ordinato per popolarità data dai link: più una pagina esiste come link nelle altre pagine, più questa deve avere un punteggio alto in classifica.

\subsubsection{Formulazione dell'algoritmo PageRank}
Sia $G = (V, E)$ un grafo pesato e orientato. Si definisce PageRank: $P \in \mathbb{R}^n$ con $n$ pari al numero di nodi del grafo tale che:

\begin{equation*}
  P_k(i) = 
    \begin{cases} 
      \displaystyle \frac{1}{n} & k = 1 \\
      \displaystyle \frac
        {\displaystyle \sum_{j=1}^{n} p_{ji}}
        {\displaystyle \sum_{\altroindice=1}^{n} {\sum_{j=1}^{n} p_{j\altroindice}}}
        P_{k-1}(j) & k > 1 

    \end{cases} 
\end{equation*}

\subsection{L'algoritmo CiteRank}
Si propone di seguito l'algoritmo proposto per il calcolo dello score per la classificazione degli autori. \\
L'algoritmo consiste nel calcolo del PageRank dei nodi utilizzando, come pesi degli archi, il numero di citazioni prima e lo stesso numero di citazioni moltiplicato per un coefficiente peggiorativo. Una volta ottenuti questi due punteggi, la loro combinazione attraverso una funzione permette di ottenere il risultato finale.
\par
Il procedimento illustrato si compone dei seguenti passi:
\begin{algorithm}
  \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{Matrice dei pesi p con $p_{ij} = \citazioni_{ij}$}
\Output{Vettore degli score $s$}
\For{$i$} {
  Calcolo del pagerank $P_{i}$ con pesi p\;
}
Calcolo della matrice dei pesi modificati 
$pmod_{ij} = p_{ij} \dot (1 - c(i, j))$\;
\For{$i$} {
  Calcolo del pagerank $\pagerankModificato_{i}$ con pesi $\pesiPagerankModificato$\;
}
\For{$i$} {
  Calcolo dello score $s_i = log_{10}(\pagerankModificato_{i}) - log_{10}(P_{i})$
}
\end{algorithm}


\begin{enumerate}
  \item 
  Al primo passo dell'algoritmo si calcola la matrice dei pesi del PageRank. \\
  Per ogni coppia di nodi $ (i, j) $, $ p_{ij} $ sarà uguale al numero di citazioni del nodo $ i $ ad articoli scritti dal nodo $ j $. 
  \item
  Si calcola poi il PageRank: 
  \begin{equation}
  P_k(i) = 
    \begin{cases} 
      \displaystyle \frac{1}{n} & k = 1 \\
      \displaystyle \frac
        {\displaystyle \sum_{j=1}^{n} p_{ji}}
        {\displaystyle \sum_{\altroindice=1}^{n} {\sum_{j=1}^{n} p_{j\altroindice}}}
        P_{k-1}(j) & k > 1 

    \end{cases} 
  \end{equation}
  dove $ k $ è il numero di iterazioni necessarie alla convergenza dell'algoritmo. L'algoritmo, basato sul metodo delle potenze (\textit{power iteration}), parte da un'approssimazione iniziale, in cui è assegnata probabilità eguale a tutti i nodi. Ad ogni iterazione aggiorna le probabilità calcolando un'approssimazione più accurata rispetto a quella dell'iterazione precedente.
  \par
  Il criterio di arresto classico dell'algoritmo consiste nella stima del miglioramento della soluzione tra l'iterazione $k$-esima e l'iterazione $k-1$-esima; ovvero nella valutazione della norma 1 tra il vettore dei PageRank all'iterazione $k$-esima e quello all'iterazione precedente:
  \begin{equation*}
    \Delta^{(k)} = \Arrowvert P^{(k)} - P^{(k+1)}\Arrowvert
  \end{equation*}
  Quando tale valore risulta essere sotto una certa soglia, si può considerare l'algoritmo giunto a convergenza.
  \par
  All'atto pratico spesso si preferisce stabilire a priori il numero di iterazioni che l'algoritmo dovrà svolgere. Questa è la scelta fatta da Neo4J e Apache Spark all'atto di implementare l'algoritmo di PageRank.
  \item
  Si passa poi a calcolare la nuova matrice di pesi per il calcolo del PageRank modificato. L'obiettivo è limitare la modifica dell'algoritmo alla sola matrice dei pesi, in modo da lasciare inalterato l'algoritmo.
  \par
  Per una famiglia di statistiche sulla collaborazione, che chiameremo $s$, la formula di calcolo della matrice di pesi $\pesiPagerankModificato$ è la seguente:
  \begin{equation}
  \begin{split}
    \pesiPagerankModificato_{ij} = 
    \left( 
      \frac
      {\displaystyle \citazioni_{ji}}
      {\displaystyle \sum_{\altroindice=1}^{n} {\citazioni_{j\altroindice}}}
    \right)
    \bullet
    \left(
      1 - 
      \sum_{h=1}^w{ 
        \alpha_h
        \frac
        {\displaystyle s^h_{ij}}
        {\displaystyle \sum_{\altroindice=1}^{n} {s^h_{i\altroindice}}}
      }
    \right) \\ \\
    \text{ con }
    \alpha \text{  t. c. } 0 \leq \alpha_h \leq 1 \text{ } \forall h \wedge \text{ } \sum_h {\alpha_h} = 1
  \end{split}
  \end{equation}
  \par
  All'atto pratico, la famiglia di statistiche utilizzate si riduce a due: il numero di citazioni ricevute dal nodo citante e il numero di collaborazioni. Il peso dell'arco da un generico nodo $i$ ad un generico nodo $j$ è calcolato, dunque, tenendo conto, oltre che del numero di citazioni del nodo $i$ ad articoli scritti dal nodo $j$, dell'indice di collaborazione tra i due autori e del numero di citazioni considerando il verso opposto, ovvero le citazioni del nodo $j$ ad articoli scritti dal nodo $i$. \\
  Questi parametri non sono considerati in senso assoluto. Bensì vengono considerati rapportati alle relazioni che il nodo citato ha con altri autori.
  Il generico peso, nel caso studiato, è definito come segue:
  \begin{equation}
  \begin{split}
    \pesiPagerankModificato_{ij} = 
    \left( 
      \frac
      {\displaystyle \citazioni_{ji}}
      {\displaystyle \sum_{\altroindice=1}^{n} {\citazioni_{j\altroindice}}}
    \right)
    \bullet
    \left(
      1 - 
      \left(
      \alpha_1
      \frac
      {\displaystyle \citazioni_{ij}}
      {\displaystyle \sum_{\altroindice=1}^{n} {\citazioni_{i\altroindice}}}
      +
      \alpha_2
      \frac
      {\displaystyle \coautori_{ij}}
      {\displaystyle \sum_{\altroindice=1}^{n} {\coautori_{i\altroindice}}}
      \right)
    \right) \\ \\
    \text{ con }
    \alpha_1 \text{ e }  \alpha_2 \text{  t. c. } 0 \leq \alpha_i \leq 1 \text{ } \wedge \text{ } \sum_i {\alpha_i} = 1
  \end{split}
  \end{equation} 

  Nella formula sopra riportata si può notare come si effettuino due diverse normalizzazioni per la parte classica del peso $p_{ij}$ e per la parte che tiene conto dell'indice di collaborazione. \\
  Il numero di citazioni del nodo $j$ al nodo $i$ sono poste in rapporto al numero di citazioni che il nodo $j$ ha rispetto a tutti gli autori. \\
  Diversamente, invece, il numero di citazioni che il nodo $i$ ha nei confronti del nodo $j$ (le citazioni considerate in senso inverso) sono pesate rispetto al numero di articoli, di qualsiasi autore, che il nodo $i$ ha citato. \\
  Stessa cosa avviene per il numero di collaborazioni. Non siamo interessati al numero di collaborazioni del nodo citante, bensì a quelle del nodo citato.
  \par
  Queste due diverse stime sono combinate attraverso una combinazione convessa dei coefficienti $\alpha_1$ e $\alpha_2$ i quali, nell'implementazione dell'algoritmo sono stati posti nel caso della media aritmetica e nei due casi limite $\alpha_1 = 0$ e $\alpha_2 = 1$ e viceversa.  

  \item 
  Ottenuta la nuova matrice di pesi, si passa al calcolo del PageRank. Per garantire l'invariante dell'algoritmo, secondo cui la somma dei pesi di una riga deve essere $1$ (rappresentando questa la somma delle probabilità di muoversi da un nodo ad un altro, probabilità che devono essere spalmate tra i diversi nodi), si effettua una seconda normalizzazione sui pesi calcolati. Questa volta la normalizzazione rimane quella dell'algoritmo classico e fa riferimento agli archi uscenti del nodo citante.
  \[  
    \pagerankModificato_k(i) = 
    \begin{cases} 
      \displaystyle \frac{1}{n} & k = 1 \\
      \displaystyle \frac
        {\displaystyle \sum_{j=1}^{n} \pesiPagerankModificato_{ji}}
        {\displaystyle \sum_{t=1}^{n} {\sum_{j=1}^{n} \pesiPagerankModificato_{jt}}}
        \pagerankModificato_{k-1}(j) & k > 1 

    \end{cases} 
  \]
  
  \item
  Ottenuti ora i due vettori $P$ e $\pagerankModificato$ il calcolo del vettore degli score avviene attraverso la differenza logaritmica:
  \[
    S_i = log(P_i) - log(\pagerankModificato_i) 
  \]

\end{enumerate}

\section{L'implementazione}
\subsection{Formato dei dati}
Per poter effettuare lo studio era necessario lavorare su dati reali. Una stima della qualità degli autori di testi scientifici con la pretesa di essere più efficace a stimare la effettiva rilevanza, deve essere avvalorata da dati reali, possibilmente in numero sufficiente affinché siano considerabili statisticamente rilevanti.
\par
In un primo momento si è pensato di utilizzare, come fonte da cui ricavare i dati, un servizio on-line di consultazione degli articoli come Scopus. \\ 
La maniera di ottenere i dati sarebbe dovuta essere incrementale.
Per incrementale si intende il non costruire l'intero dataset a priori dell'analisi dei nodi del grafo, bensì costruire il sotto-grafo "centrato" rispetto ad un autore.
Quindi, scegliendo un autore, si analizzano i suoi articoli, le referenze di tali articoli, ed infine dagli autori delle referenze è possibile stabilire un collegamento tra i due autori. \\
L'approccio descritto possiede almeno due aspetti negativi non trascurabili,
La procedura ha una complessità elevata e richiede una ripetutamente l'interrogazione di un servizio online, quale Scopus, che vieta espressamente le pratiche di Data Mining attraverso l'utilizzo delle loro API. \\
Dunque si è cercata una strada differente che permettesse di analizzare localmente una grande quantità di dati. \\
Come scelta definitiva si è utilizzato un dataset ospitato dal sito aminer.org. \\
Tale sito si prefigge l'obiettivo di riunire i dati di quanti più articoli possibili, relativamente a pubblicazioni del macro-ambito della computer science.
aminer.org aggiorna ogni anno il suo dataset, rilasciandone una nuova versione. Nella versione 10 (la versione di cui si fa riferimento in questo lavoro) possiede un numero di poco superiore ai 3 milioni di articoli. \\
\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/aminer.jpg}
  \caption{Il sito web dell'organizzazione aminer.org}
\end{figure}

Il dataset presenta i dati nel formato JSON. Contiene un diverso oggetto JSON per ogni articolo. Gli oggetti sono disposti nel file suddivisi per riga.
Per ogni articolo sono presenti i seguenti campi: \\
\begin{tabularx}{\textwidth}{| X | X | X | X |}
\hline \textbf{Nome Campo} & \textbf{Tipo Campo} & \textbf{Descrizione} & \textbf{Esempio} \\
\hline id & stringa & id della pubblicazione & ``013ea675-bb58-42f8-a423-f5534546b2b1''\\
\hline title & stringa & titolo della pubblicazione & ``Dynamic analysis of a pest-epidemic model with impulsive control'' \\
\hline authors & lista di stringhe & autori della pubblicazione & [``Leon A. Sakkal'', ``Kyle Z. Rajkowski'', ``Roger S. Armen'']\\
\hline venue & stringa & sede di pubblicazione & ``Neural Networks'' \\
\hline year & intero & anno di pubblicazione & 1994 \\
\hline n\_citation & intero & sede di pubblicazione & 24 \\
% \hline references & stringa & lista delle citazioni & [``013ea675-bb58-42f8-a423-f5534546b2b1'', ``c043e258-256f-4957-8ac5-899905eb4efd'']\\
% \hline abstract & stringa & sommario & ``In this work we propose schemes for joint modelorder and step-size adaptation of reduced-rank adaptive filters \ldots''\\
\hline
\end{tabularx}

Di seguito si mostra un esempio di oggetto JSON del dataset rappresentante un articolo:
\begin{lstlisting}[keepspaces=true]
{
  "authors": [
    "Leon A. Sakkal",
    "Kyle Z. Rajkowski",
    "Roger S. Armen"
  ],
  "n_citation": 0,
  "references": [
    "4f4f200c-0764-4fef-9718-b8bccf303dba",
    "aa699fbf-fabe-40e4-bd68-46eaf333f7b1"
  ],
  "title":
      "Prediction of consensus binding mode geometries
      for related chemical series of positive allosteric
      modulators of adenosine and muscarinic acetylcholine
      receptors",
  "venue": "Journal of Computational Chemistry",
  "year": 2017,
  "id": "013ea675-bb58-42f8-a423-f5534546b2b1"
}
\end{lstlisting}
\subsection{Il preprocessing dei dati}
Il formato JSON, sebbene pratico ed efficace per lo sviluppo, ad esempio, di API di servizi online e per la memorizzazione di dati senza una precisa strutturazione, non si presta ad elaborazione di grandi quantità di dati. Questo per almeno due motivi:
\begin{enumerate}
  \item Spreca molti dati per la memorizzazione dei caratteri di controllo (inizio/fine di un oggetto o di una lista, virgolette di apertura/chiusura delle stringhe). Inoltre aggiunge le stringhe contenenti i nomi dei campi per ogni oggetto, anche se gli oggetti si riferiscono ad uno stesso schema.
  \item L'interpretazione di un oggetto non è un'operazione banale come può essere quella di distinguere i campi di un file csv (comma separated values). La struttura ricorsiva del JSON richiede della computazione che per dataset di dimensione di diversi Gigabyte può diventare un overhead rilevante.
\end{enumerate}

Per ovviare a tali problemi si è deciso di agire con un primo preprocessing. Si sono filtrati i soli dati rilevanti agli scopi del lavoro in oggetto. In particolare, per ogni articolo, si sono mantenute le sole informazioni dell'id dell'articolo, la lista degli autori e la lista delle referenze. Si è sostituito, inoltre, il dispendioso formato JSON con un formato CSV leggermente modificato per poter distinguere le liste degli autori e delle referenze.
\par
Il formato scelto per la memorizzazione si presenta così:
\begin{lstlisting}[keepspaces=true]
id_articolo,ref_1;ref_2,autore_1;autore_2;autore_2
\end{lstlisting}
Si è memorizzato un articolo per ogni riga. In questo modo è possibile utilizzare il carattere fine-linea per distinguere gli articoli tra loro.
Separando le stringhe utilizzando come separatore la virgola si ottengono dunque tre stringhe contenenti:
\begin{itemize}
  \item l'id
  \item la lista degli id delle referenze
  \item la lista dei nomi degli autori
\end{itemize}
Le due liste suddette si possono ora separare utilizzando come separatore il punto e virgola, ottenendo infine gli autori e gli articoli referenziati distinti.

A questo punto l'esempio di oggetto articolo si riduce a:
\begin{lstlisting}
013ea675-bb58-42f8-a423-f5534546b2b1,
4f4f200c-0764-4fef-9718-b8bccf303dba;
aa699fbf-fabe-40e4-bd68-46eaf333f7b1,
Leon A. Sakkal,Kyle Z. Rajkowski,Roger S. Armen
\end{lstlisting}

Non essendo interessante al fine di quantificare il numero di citazioni, gli id degli articoli sono stati sostituiti da degli identificativi numerici. Questo sia per comprimere i dati senza perdere le informazioni utili, sia per velocizzare il successivo trasporto degli articoli su una struttura dati array.
In seguito alla trasformazione le righe si presentano così:
\begin{lstlisting}
123,425;2345,Leon A. Sakkal,Kyle Z. Rajkowski,
Roger S. Armen
\end{lstlisting}
A partire da tale preprocessing si è passati alla importazione e alla creazione degli effettivi grafi delle citazioni e delle collaborazioni.


Per l'implementazione finale è stata fatta la scelta di avere un'implementazione custom dell'algoritmo PageRank e dei suoi derivati. \\
Tale scelta è stata dettata dal fatto che il livello di prestazioni ottenibili utilizzando le basi di dati a grafo non erano ragionevoli per eseguire il lavoro sulla mole di dati a disposizione.
\subsection{Il linguaggio Scala}
L'implementazione è stata svolta nel linguaggio di programmazione Scala \cite{scalalang}. \\
Il linguaggio Scala, linguaggio staticamente tipizzato basato sulla Java Virtual Machine, offre, oltre ad una sintassi espressiva e conveniente, strutture dati che rendono semplice il calcolo parallelo; calcolo parallelo molto utile per ridurre notevolmente i tempi di calcolo nelle macchine multicore.
\subsection{La generazione del grafo}
Per la generazione del grafo a partire dalla versione filtrata e compattata del dataset si è utilizzato un metodo multi-fase.
\begin{enumerate}
  \item In primo luogo è stato necessario ottenere gli autori distinti e ordinati. L'ordinamento è necessario per l'utilizzo dell'algoritmo di ricerca binaria. Per ottenere gli autori distinti e ordinati si è, in un primo momento, utilizzata una struttura dati TreeSet, struttura dati che naturalmente mantiene l'ordinamento degli oggetti. Una volta inserite le stringhe contenenti i nomi degli autori nel TreeSet è sufficiente, infatti, convertire l'insieme in un array per ottenere la lista ordinata. In un secondo momento si è optato per una soluzione più efficiente. Costruendo prima un ArrayBuilder (struttura dati che mantiene un array dinamicamente crescente per la costruzione degli array), calcolando gli elementi distinti ed infine applicando l'algoritmo quickSort per ordinare l'array. Questa ottimizzazione ottiene un miglioramento delle performance di quasi un ordine di grandezza.
  \begin{lstlisting}[keepspaces=true]
def authorsArray(file: Array[Line],
                 numLines: Int):
    Array[String] = {
  def builder() = {
    val res =
      mutable.ArrayBuilder.make[String]()
    cfor(0)(_ < numLines, _ + 1) { i =>
      val line = file(i)
      for (author <- line.authors) {
        res += author
      }
    }
    res
  }
  val authors = builder().result.distinct
  util.Sorting.quickSort(authors)
  authors
}    
  \end{lstlisting}
  \item Una volta ottenuti gli autori distinti il passo successivo è stato quello di creare una struttura dati per mantenere le informazioni sulle relazioni. Si tratta di una matrice rappresentata come matrice sparsa. La rappresentazione come matrice sparsa è utile perché, per ogni autore, il numero di collaborazioni e di citazioni è piuttosto basso. Il costo quadratico di una rappresentazione matriciale sarebbe stato inutile e computazionalmente costosissimo. La struttura dati usata è stata, dunque, un Array di ArrayBuffer di Array di interi. Il primo array contiene alla posizione $i$-esima la lista delle citazioni ricevute, all'interno di un ArrayBuffer, struttura dati del linguaggio Scala rappresentante un array di dimensione variabile. Tale lista contiene degli Array di interi di dimensione fissa contenenti la posizione dell'autore citante nella matrice, e la lista delle statistiche (numero di coautori, numero di citazioni, numero di citazioni ad articoli scritti come da coautori, ecc.).
  \begin{lstlisting}[keepspaces=true]
val relazioni: 
  Array[mutable.ArrayBuffer[Array[Int]]] = 
  Array.fill(authors.length)
            (mutable.ArrayBuffer.empty)

var autoriArticolo =
  new Array[Array[Int]](numArticoli)
cfor(0)(_ < numLines, _ + 1) { index =>
  val line = file(index)
  for (ref <- line.references)
    if (autoriArticolo(ref) == null)
      autoriArticolo(ref) = Array.empty
  val authsIdx: Array[Int] =
    line.authors.map(a =>
      binarySearch(authors, a)
    )
  autoriArticolo(line.id) = authsIdx
}
  \end{lstlisting}
  \item Un'altra struttura dati utilizzata è quella che mantiene, per ogni articolo, la lista degli indici dei suoi autori. Per riempire questa seconda matrice è stata utilizzato l'algoritmo di ricerca binaria.
  Per ogni riga del file e per ogni autori di tale articolo si è utilizzata la ricerca binaria per ottenere l'indice di riga a partire dalla stringa del nome.
	  \item Una terza scansione, infine è stata necessaria per riempire la matrice delle relazioni. Scandendo gli articoli, per ogni articolo si sono presi gli autori, e la stessa cosa è stata fatta per gli autori degli articoli citati. A questo punto è stata aggiunta una entry nella lista delle citazioni per gli autori non ancora citati, è stato, invece, incrementato il contatore delle citazioni per gli autori già citati. Per avere, successivamente, la possibilità di una migliore scrematura delle citazioni, sono state suddivise in diverse categorie:
  \begin{enumerate}
    \item citazioni da un autore A ad un autore B in cui non ci sono relazioni tra A e B per quanto riguarda gli articoli in oggetto.
    \item citazioni da un autore A ad un autore B in cui l'autore dell'articolo citante (l'autore A) è anche coautore dell'articolo citato (coautore di B).
    \item citazioni da un autore A ad un autore B in cui l'autore dell'articolo citato (l'autore B) è anche coautore dell'articolo citante (coautore di A).
    \item citazioni da un autore A ad un autore B in cui in cui entrambi gli autori sono coautori degli articoli citante e citato. 
  \end{enumerate}
  \begin{lstlisting}[keepspaces=true]
cfor(0)(_ < numLines, _ + 1) { index =>
  val line = file(index)
  for (ref <- line.references; 
      citer <- autoriArticolo(line.id);
      cited <- autoriArticolo(ref)) {
    val tipo =
      if (cited != citer) {
        val isCitaComeCoautoreA = 
          autoriArticolo(ref).contains(citer)
        val isCitaComeCoautoreB = 
          autoriArticolo(line.id)
            .contains(cited)

        if (isCitaComeCoautoreA &&
            isCitaComeCoautoreB)
          CitaComeCoautoreAAndB
        else if (isCitaComeCoautoreA)
          CitaComeCoautoreA
        else if (isCitaComeCoautoreB)
          CitaComeCoautoreB
        else
          Cita
      } else Cita

    incrementValueIn(relazioni(citer), 
                     cited,
                     tipo)
  }
}  
\end{lstlisting}
Una volta generato il grafo con tutte le informazioni necessarie è stato esportato in un formato testuale come lista di archi per essere processato dall'algoritmo di PageRank.
Il formato utilizzato è stato una lista di numeri separati da spazi secondo tale convenzione:
\begin{lstlisting}[keepspaces=true]
autoreCitante autoreCitato statistica1 ...
\end{lstlisting}
Questo file sarà importato dal programma di PageRank ad ogni test.
\end{enumerate}
\subsection{L'implementazione del PageRank}
A valle della generazione del grafo (operazione maggiormente onerosa) c'è stata la vera e propria implementazione dell'algoritmo PageRank.
A differenza della generazione del grafo che è un'operazione svolta una tantum, l'algoritmo di PageRank deve essere eseguito diverse volte con parametri diversi, per valutarne i risultati. Dunque la velocità di esecuzione risulta essere maggiormente importante in questo caso. Per ottenere le performance migliori possibili, oltre ad utilizzare codice ottimizzato utilizzando strutture dati di livello più basso possibile (array) e strutture di controllo imperative a basso livello (while) al posto di paradigmi più eleganti da un punto di vista del codice ma meno performanti, è stato utilizzato il parallelismo per rendere parallele tutte le operazioni più onerose che lavorano su insiemi distinti.
\subsubsection{Computazione parallela}
La parallelizzazione del codice è stata enormemente semplificata dalla scelta progettuale di utilizzare il linguaggio Scala. Questo possiede un costrutto molto comodo, chiamato ``Parallel Range'' \cite{parallel} che permette l'iterazione su un range numerico in modo parallelo.
Ad esempio, si noti questo codice:
\begin{lstlisting}[keepspaces=true]
val a = new Array[Int](1000)
val b = new Array[Int](1000)
// Inizializzazione array ...
val c = new Array[Int](1000)
for(i <- 0.to(1000)) {
  c(i) = a(i) + b(i)
}
\end{lstlisting}
Una volta creati 3 array da 1000 elementi, per ogni indice $i$ inserisce la somma dei valori dei primi due array in quell'indice nel terzo array. \\
Questo è un esempio di codice parallelizzabile. Infatti il risultato della somma non dipende dai valori degli altri indici diversi da $i$. \\
Per rendere parallelo il codice è necessario rendere il ``Range'' $0.to(1000)$ un range parallelo chiamando il metodo $par$. Tale metodo suddivide il range in parti uguali in numero eguale al numero dei processori disponibili sulla macchina e manda in esecuzione il blocco di codice a thread diversi.
\subsubsection{Lettura parallela da file}
Anche la lettura del file del grafo è stata resa parallela grazie a principio appena esposto. Per evitare la suddivisione del file in lettura è stato fatto un preprocessing del file: attraverso l'utility bash ``split'' \cite{split} si è suddiviso questo per righe in 8 parti uguali (il numero di processori della macchina) e si è fatta una lettura parallela dei files utilizzando il range parallelo precedentemente esposto:
\begin{lstlisting}[keepspaces=true]
for (lines <- filesLines.par; line <- lines) {
  val st = split(line, ' ')
  val citante = st(0).toInt
  val citato = st(1).toInt
  val coautori = st(2).toShort
  val citazioni =
    (st(3).toShort + st(5).toShort).toShort
  locks(citato).synchronized {
    if (citatiCreazione(citato) == null) {
      citatiCreazione(citato) =
        mutable.ArrayBuilder.make[Int]
      citazioniCreazione(citato) = 
        mutable.ArrayBuilder.make[Short]
      coautoriCreazione(citato) =
        mutable.ArrayBuilder.make[Short]
    }
    citatiCreazione(citato) += citante
    coautoriCreazione(citato) += coautori
    citazioniCreazione(citato) += citazioni
  }
}
for (file <- files) file.close()
\end{lstlisting}
Per evitare problemi di race conditions è stato utilizzato un array di lucchetti utilizzando i monitor nativi di Java.
\subsubsection{L'implementazione del PageRank}
Lo stesso algoritmo PageRank è stato implementato massimizzando il parallelismo. L'algoritmo ha una complessità trattabile, il che lo rende, all'atto pratico, veloce da computare, anche su grafi di grandi dimensioni. La complessità temporale dell'algoritmo è, infatti, lineare rispetto al numero di archi.
\par
L'algoritmo utilizza una struttura dati parziale che serve alla normalizzazione. \`E necessario calcolare la somma dei pesi di tutti gli archi uscenti da ogni nodo. Questa operazione viene fatta a monte del calcolo del PageRank. Per questo esiste una funzione pubblica denominata ``pageRank'' che calcola l'array ``sommaPesiUscenti'' e poi chiama la funzione privata ``pageRankImpl'' contenente l'implementazione:
\begin{lstlisting}[keepspaces=true]
def pageRank(citati: Matrice[Int],
              pesiPageRank: Matrice[Double],
              iterations: Int,
              dampingFactor: Double = 0.85):
                Array[Double] = {
  val sommaPesiUscenti =
    Array.fill[Double](citati.length)(0.0)

  cfor(0)(_ < citati.length, _ + 1) { i =>
    if (citati(i) != null) {
      val cI = citati(i)
      val cILength = cI.length
      cfor(0)(_ < cILength, _ + 1) { j =>
        val citante = cI(j)
        sommaPesiUscenti(citante) +=
          pesiPageRank(i)(j)
      }
    }
  }

  pageRankImpl(citati,
                pesiPageRank,
                iterations,
                dampingFactor,
                sommaPesiUscenti)
}
\end{lstlisting}
La funzione scandisce gli archi del grafo computando la somma dei pesi degli archi uscenti dal nodo citante.
Di seguito si mostra, invece, l'implementazione della funzione che esegue il calcolo vero e proprio del PageRank:
\begin{lstlisting}[keepspaces=true]
def pageRankImpl(citati: Matrice[Int],
                 pesiPageRank: Matrice[Double],
                 iterations: Int,
                 dampingFactor: Double = 0.85,
                 sommaPesiUscenti: Array[Double]):
                   Array[Double] = {

  val pageRanksPrevIteration: Array[Double] =
    Array.fill(citati.length)(1.0 / citati.length)
  val pageRanks: Array[Double] =
    Array.fill(citati.length)(0.0)
  val damp: Double = 
    (1.0 - dampingFactor) / citati.length

  cfor(0)(_ < iterations, _ + 1) { iteration =>
    for (i <- citati.indices.par;
        if citati(i) != null) {
      var pageRankI: Double = 0.0
      val cI = citati(i)
      val cILength = cI.length
      val prI = pesiPageRank(i)
      cfor(0)(_ < cILength, _ + 1) { j =>
        val citante = cI(j)
        val somma = sommaPesiUscenti(citante)
        val pesoNormalizzato: Double =
          if (somma == 0.0) 0.0 else prI(j) / somma
        pageRankI += 
          pageRanksPrevIteration(citante) *
            pesoNormalizzato
      }
      pageRanks(i) = 
        damp + dampingFactor * pageRankI
    }
    cfor(0)(_ < pageRanks.length, _ + 1) { i =>
      pageRanksPrevIteration(i) = pageRanks(i)
    }
  }
  pageRanks
}
\end{lstlisting}
Questa inizializza il vettore dei pagerank assegnando probabilità distribuite uniformemente tra i nodi.
Poi itera il procedimento per un numero prefissato di iterazioni. Ad ogni iterazione computa il valore attuale del vettore dei pagerank utilizzando i valori dell'iterazione precedente. Tra un'iterazione e l'altra viene effettuata la copia del vettore ``pageRanks'' nel vettore ``pageRanksPrevIteration''.
\par
Il fatto che il valore del pagerank $i$-esimo dipenda solo dal vettore della precedente iterazione (costante nel momento del calcolo) e dalla matrice dei pesi (anch'essa costante), rende l'algoritmo parallelizzabile.
Quindi, utilizzando un range parallelo, si itera sui nodi. Per ogni nodo si itera sui suoi archi entranti sommando al pagerank del nodo il pagerank della precedente iterazione del noto citante moltiplicato per il peso normalizzato sui suoi pesi uscenti.
\par
Una volta ottenuto il valore di pagerank se ne fa la media pesata con la probabilità di teletrasporto, che è uniformemente distribuita tra tutti i nodi. Il peso di tale media è il cosiddetto ``damping factor'', che sperimentalmente viene posto, in molte implementazioni, pari a 0.85. 

\subsection{L'implementazione del PageRank modificato}
Come detto in precedenza, il PageRank modificato non fa altro che agire a monte dell'algoritmo di PageRank, creando una matrice dei pesi che dà in pasto all'implementazione classica.
Dalla firma della funzione si può notare che essa riceve, oltre alla matrice sparsa dei citati, ovvero la matrice contenente gli id dei nodi che hanno almeno una citazione nel nodo in oggetto, le matrici che contengono le statistiche prese in esame dall'algoritmo modificato.
\begin{lstlisting}
def pageRankMod(citati: Matrice[Int],
  citazioni: Matrice[PesiType],
  coautori: Matrice[PesiType],
  sommaCoautori: Array[PesiType],
  iterazioni: Int,
  dampingFactor: Double = 0.85): Array[Double] = {
\end{lstlisting}
Poi calcola la somma delle citazioni uscenti da ogni nodo (necessarie per la normalizzazione):
\begin{lstlisting}
  val numAutori = citati.length
  val sommaCitazioniDate = 
    new Array[PesiType](numAutori)

  cfor(0)(_ < numAutori, _ + 1) { i =>
    if (citati(i) != null) {
      cfor(0)(_ < citati(i).length, _ + 1) { j =>
        val citante = citati(i)(j)
        sommaCitazioniDate(citante) =
          (sommaCitazioniDate(citante) + 
            citazioni(i)(j)).toShort
      }
    }
  }
\end{lstlisting}
Tali somme sono usate nella normalizzazione delle citazioni uscenti ed entranti da ogni nodo:
\begin{lstlisting}  
  val citazioniDateNormalizzate: Matrice[Double] =
    new Matrice[Double](numAutori)
  val citazioniRicevuteNormalizzate:
      Matrice[Double] =
    new Matrice[Double](numAutori)
  val coautoriNormalizzati: Matrice[Double] =
    new Matrice[Double](numAutori)

  for (i <- citati.indices.par) {
    if (citati(i) != null) {
      citazioniDateNormalizzate(i) =
        Array.fill[Double](citati(i).length)(0.0)
      citazioniRicevuteNormalizzate(i) =
        Array.fill[Double](citati(i).length)(0.0)
      coautoriNormalizzati(i) =
        Array.fill[Double](citati(i).length)(0.0)

      val cI = citati(i)
      val cILength = cI.length
      cfor(0)(_ < cILength, _ + 1) { j =>
        val citante = cI(j)
        citazioniRicevuteNormalizzate(i)(j) = {
          val n = sommaCitazioniDate(citante)
          if (n == 0.toShort) 
            0.0 
          else 
            citazioni(i)(j).toDouble / n
        }
        citazioniDateNormalizzate(i)(j) = {
          val n = sommaCitazioniDate(i)
          if (n == 0.toShort) 0.0
          else {
            val ci = citati(citante)
            if (ci != null) {
              def met: Double = {
                cfor(0)(_ < ci.length, _ + 1) { k =>
                  if (ci(k) == i) {
                    return citazioni(citante)(k)
                        .toDouble / n
                  }
                }
                0.0
              }
              met
            } else 0.0
          }
        }
\end{lstlisting}
Stessa cosa avviene per la normalizzazione del numero di collaborazioni. Il numero di collaborazioni viene, infatti, normalizzato rispetto al numero di collaborazioni totali del nodo citato.
\begin{lstlisting} 
        coautoriNormalizzati(i)(j) = {
          val n = sommaCoautori(i)
          if (n == 0.toShort) 
            0.0 
          else 
            coautori(i)(j).toDouble / n
        }
      }
    }
  }
\end{lstlisting}
Calcolati tutti i parametri normalizzati si procede al calcolo degli effettivi pesi da dare in pasto all'algoritmo pageRank:
\begin{lstlisting}
  val pesiPageRank = new Matrice[Double](numAutori)
  for (i <- citati.indices.par; 
    if citati(i) != null) {
      pesiPageRank(i) = 
        Array.fill[Double](citati(i).length)(0.0)

    val cI = citati(i)
    val cILength = cI.length
    cfor(0)(_ < cILength, _ + 1) { j =>
      pesiPageRank(i)(j) =
        citazioniRicevuteNormalizzate(i)(j) * 
          (1 - 
            (coautoriNormalizzati(i)(j) +
            citazioniDateNormalizzate(i)(j)) / 2.0 )
    }
  }
\end{lstlisting}
Infine si procede chiamando la funzione pageRank con l'implementazione classica:
\begin{lstlisting}
  pageRank(citati,
           pesiPageRank,
           iterazioni,
           dampingFactor)
}
\end{lstlisting}

\section{Analisi dei dati}
Utilizzando gli algoritmi descritti negli scorsi capitoli si sono svolte le analisi dei dati sperimentali.

In primo luogo si sono messe in relazione le due misure del PageRank. Il risultato atteso era quello di avere due misure per cui fossero pochi i nodi con una variazione consistente. Questo perché il PageRank proposto non è che una variazione del PageRank classico, non un suo totale stravolgimento. Non ci si aspetta, quindi, che un autore con un dato punteggio abbia un pagerank modificato drasticamente diverso.
\begin{figure}[H]
  \includegraphics[width=0.7\linewidth]{images/grafico-1.png}
  \caption{Scatter plot dei due PageRank messi in relazione}
\end{figure}
\subsection{Rappresentazioni dello score}
Guardando il grafico precedentemente, che mostra come lo scostamento delle due varianti del PageRank sia molto basso per la maggior parte dei nodi, ci si aspetta che la loro differenza si discosti poco dallo zero. Anche per questo motivo si è scelto l'utilizzo della differenza logaritmica come funzione di scoring. Perché si vuole dare stesso peso alle variazioni di pagerank a prescindere dall'ordine di grandezza considerato. \\
Si mostra, dunque, la relazione tra valori di PageRank crescenti e lo score relativo su scala logaritmica.
\begin{figure}[H]
  \includegraphics[width=0.8\linewidth]{images/grafico-2.png}
  \caption{Scatter plot dei due PageRank messi in relazione}
\end{figure}
Il valore del PageRank, pur non essendo linearmente proporzionale al numero di archi entranti (sappiamo che viene calcolato in base al PageRank dei nodi da cui provengono i suoi archi entranti), è strettamente legato al numero di archi entranti.
\par Vediamo come varia lo score rispetto al numero totale di citazioni di ogni autore:
\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/grafico-3.png}
  \caption{Scatter plot dei due PageRank messi in relazione}
\end{figure}

\subsection{Considerazioni}
A partire dai grafici precedentemente esposti si possono fare alcune considerazioni.
\subsubsection{Comportamento dello score nei nodi con PageRank elevato}
Si può notare come i nodi con un grande numero di citazioni tendono ad avere uno score mediamente inferiore allo zero. Ci si aspetta un comportamento simile perché è inverosimile che un grandissimo numero di citazioni possa provenire dalla rete di collaborazione di un nodo. L'algoritmo, quindi, attenuando il punteggio dei nodi in cui le citazioni provenienti dalla rete di collaborazione sono numericamente predominanti sul numero di citazioni totali, cede le quantità di pagerank prelevate agli altri nodi.
\begin{figure}[H]
  \includegraphics[width=0.6\linewidth]{images/grafico-4.png}
  \caption{Tendenza dei nodi con valori di PageRank alti}
\end{figure}

\subsubsection{Analisi degli outliers}
Analizzando alcuni dei nodi outliers si possono fare alcune considerazioni:
\begin{figure}[H]
  \includegraphics[width=0.7\linewidth]{images/grafico-5.png}
  \caption{Scelta degli outliers considerando la funzione di scoring logaritmica}
\end{figure}
\medskip
\begin{enumerate}
  \item Considerando i nodi outliers nel semipiano positivo presi tra i nodi aventi un punteggio pagerank alto, è possibile individuare nodi la cui rete di collaborazione offre un contributo rilevante nell'apporto di citazioni: \\
  preso come esempio il nodo con $\#1$, esso possiede un punteggio PageRank elevato, pari a $9.89$x$10^{-5}$ e pagerank modificato pari a $6.45$x$10^{-5}$. Il suo score si distingue da quelli con suo stesso ranking e da quelli con PageRank simile. Questo perché esso possiede molte citazioni nei confronti di molti dei suoi nodi citanti. Possiede citazioni a molti autori che sono o suoi coautori o che hanno citato suoi articoli.
  \item Essere collaboratori di un nodo con pagerank alto ha l'effetto, secondo la semantica dell'algoritmo originale, di ottenere un incremento notevole del proprio punteggio.
  Preso in esame il nodo con score (differenza logaritmica) massimo, il nodo $\#2$, si scopre che esso ha un'autocitazione e una citazione da autore con pagerank dell'ordine di $10^{-6}$. Questo fa schizzare il suo PageRank anche nell'ordine di $10^{-6}$. Il PageRank modificato, invece, non considerando le autocitazioni e dando contributo peggiorativo alla citazione del nodo nei confronti del nodo citante, abbassa il suo pagerank nell'ordine di $10^{-8}$, pagerank che avrebbe dovuto avere senza la citazione proveniente dalla sua collaborazione.
  \item Il peggioramento dello score si espande sui vicini. Per come è strutturato l'algoritmo PageRank, esso si basa sul pagerank dei vicini per valutare il pagerank del nodo in oggetto. Va da sè che se il pagerank dei vicini di un nodo è molto variato per un rilevante numero di citazioni nella rete di collaborazione, anche il pagerank di un nodo che lavora in autonomia viene peggiorato. Questa condizione non viola gli obiettivi dell'algoritmo. Se si vuole stimare la visibilità degli autori, la diminuzione di visibilità di un autore deve ripercuotersi sugli autori da egli citati, a prescindere della loro rete di collaborazione. \\
  Questa condizione, tuttavia, si osserva per pochi outliers.
  \item L'algoritmo non considera negativa la collaborazione di per sè; le citazioni dei coautori, infatti, sono considerate nel coefficiente peggiorativo, solo quando sono numerose in proporzione al numero totale di collaborazione. Una coppia di collaboratori, le cui citazioni reciproche costituiscono solo una piccola parte delle citazioni totali, non sono danneggiati nello score.
\end{enumerate}

\section{Sviluppi futuri}
Il lavoro proposto mette in luce come, valutando quanto la rete collaborazione degli autori influisce nelle citazioni, ci si può avvicinare a valutare la reale visibilità degli autori.
Attraverso una procedura ``qualitativa'' è possibile individuare gli outliers e verificare che corrispondono al profilo designato nella stesura dell'algoritmo. \\
Tuttavia è assente un procedimento univoco per l'identificazione degli outliers. Tale identificazione si potrà ottenere in due modi: \begin{enumerate}
  \item rendendo la funzione di scoring, al momento pari alla differenza logaritmica, tale da dare peso costante agli outliers presenti in ogni ordine di grandezza di pagerank e di numero di citazioni.
  \item definendo una seconda funzione matematica capace di separare univocamente i veri outliers, curva che è stata rappresentata in maniera qualitativa nell'analisi dei risultati precedente.
\end{enumerate}
Un altro possibile sviluppo è l'estensione dello studio ad altri database, come il database di bibliometrico PubMed relativo alla letteratura biomedica, per verificare la presenza degli stessi pattern e verificare, dunque, che i risultati esposti non sono frutto della configurazione del particolare caso preso in oggetto.

\bibliographystyle{unsrt}
\bibliography{tesi}
\end{document}